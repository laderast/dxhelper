{"metadata": {"language_info": {"name": "R", "codemirror_mode": "r", "pygments_lexer": "r", "mimetype": "text/x-r-source", "file_extension": ".r", "version": "4.2.0"}, "kernelspec": {"name": "ir", "display_name": "R", "language": "R"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Using Spark/`{sparklyr}` to extract datasets\n\nIf our query is takes longer than 2 minutes to execute, `dx extract_datasets` will not run. In that case, we will start up a Spark instance of JupyterLab and run the SparkSQL directly in the notebook.\n\n1. Extract the relevant dictionary information\n2. Connect to our Spark Cluster using `sparklyr`\n3. Extract the dataset SQL using the `--sql` option with `dx extract_dataset` and save output to a file\n4. Load query from file, clean the SQL\n5. Use `dbGetQuery` to load the dataset as a `data.frame`\n6. Use `readr::write_csv()` to save `data.frame` as CSV file\n7. Upload our CSV file back to project storage using `dx upload`.", "metadata": {}}, {"cell_type": "code", "source": "#install.packages(\"sparklyr\")\n", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Generate SQL from `dx extract_dataset`\n\nIf we use the `--sql` flag, the query will not execute, but the SparkSQL needed to execute the query will be saved. \n\nWe'll use the `-o` option to save our query as `cohort.sql`.", "metadata": {}}, {"cell_type": "code", "source": "system(\"dx extract_dataset record-G5Ky4Gj08KQYQ4P810fJ8qPp -ddd\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "field_list <- 'participant.eid,participant.p21022,participant.p1508_i0,participant.p1508_i1,participant.p1508_i2,participant.p41202,participant.p31,participant.p41226,participant.p3159_i0,participant.p3159_i1,participant.p3159_i2'\n\ncohort <- \"record-G5Ky4Gj08KQYQ4P810fJ8qPp\"\n\ncohort_template <- glue::glue(\"dx extract_dataset {cohort} --fields {field_list} --sql -o cohort.sql\")\n\ncohort_template\n\nsystem(cohort_template)\nlist.files()", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Connect to Spark Cluster\n\nThe first thing we need to do is connect to the Spark Cluster. We'll use the `{sparklyr}` and `{DBI}` packages to connect.\n\nMake sure to only connect once - Spark will throw an error if you try to connect twice by rerunning this code block.\n\nTo fix this, you will need to restart the kernel using **Kernel > Restart Kernel*", "metadata": {}}, {"cell_type": "code", "source": "library(sparklyr)\nlibrary(DBI)\nport <- Sys.getenv(\"SPARK_MASTER_PORT\")\nmaster <- paste(\"spark://master:\", port, sep = '')\nsc = spark_connect(master)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Load SQL from file\n\nWe'll read the `cohort.sql` file using `read_file()` and then execute the SQL using `dbGetQuery()`.\n\nHere's what the actual query looks like:", "metadata": {}}, {"cell_type": "code", "source": "library(readr)\nretrieve_sql <-read_file(\"cohort.sql\")\nretrieve_sql <- gsub(\"[;\\n]\", \"\", retrieve_sql)\nretrieve_sql", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Execute Our Query with DBI/sparklyr\n\nNow we can execute our SQL with `dbGetQuery()`. Note we utilize our Spark Connection (`sc`) to execute it.\n\nNote that the `data.frame` returned by `dbGetQuery()` is not identical to the `.csv` file generated by `dx extract_dataset`. \n\n`dbGetQuery()` returns columns that have multiple values per lines as list columns. `{xvhelper}` can transform these columns into comma separated strings.", "metadata": {}}, {"cell_type": "code", "source": "df <- dbGetQuery(sc, retrieve_sql)\nhead(df)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "library(xvhelper)\n\n    \n  data_dict <- readr::read_csv(\"female_coffee_3.0.data_dictionary.csv\", show_col_types = FALSE)\n  coding_dict <- readr::read_csv(\"female_coffee_3.0.codings.csv\", show_col_types=FALSE)\n    \n\n\nmerged_dict <- xvhelper::merge_coding_data_dict(coding_dict, data_dict)\n\ndecoded <- df |> \n decode_df(merged_dict) \n", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "head(decoded)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Decode Column Names\n\nWe can also decode the column names:", "metadata": {}}, {"cell_type": "code", "source": "decoded_cols <- decoded |>\n    xvhelper::decode_column_names(merged_dict)\n\nhead(decoded_cols)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Working with Data in Spark\n\nWe can also load our data as a temporary table in Spark, which can be very useful when working with very large datasets.\n\nThis lets us leverage the power of Spark and the `{sparklyr}` package.\n\nWe'll first take our SQL query and use `CREATE TEMPORARY VIEW` to create a temporary View in Spark.", "metadata": {}}, {"cell_type": "code", "source": "temp_table_sql <- glue::glue(\"CREATE TEMPORARY VIEW pheno AS {retrieve_sql}\")\n\nresult <- DBI::dbSendQuery(sc, temp_table_sql)\n#sparklyr::sdf_register(temp_table_sql,sc)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Now we can register the data frame (`pheno`) as a Spark dataframe using `dplyr::tbl()`. \n\nOnce we do that, we're able to leverage `sparklyr` to do `dplyr` like filtering:", "metadata": {}}, {"cell_type": "code", "source": "new_df <- dplyr::tbl(sc, \"pheno\")\n\nfiltered_df <- new_df |> dplyr::filter(`participant.p21022` > 50)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "head(filtered_df)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "colnames(new_df)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "new_df |> \n    dplyr::mutate(`participant.p31` = as.character(`participant.p31`)) |>\n    dplyr::count(`participant.p31`)\n\n#|>\n#    dplyr::group_by(`participant.p31`) |> summarise(mean_age = mean(`participant.p21022`, na.rm=TRUE))", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Once we're done with working with `filtered_df` in spark, we can pull it into memory using `collect()`. Then we can annotate the data as usual.", "metadata": {}}, {"cell_type": "code", "source": "in_memory <- collect(filtered_df)\nhead(in_memory)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}]}
