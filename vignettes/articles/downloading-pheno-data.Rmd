---
title: "Downloading Pheno Data in JupyterLab/RStudio"
---

> Note: This functionality is currently experimental. I'm working to get it working on UK Biobank RAP right now.

## Using `xvhelper` to download and decode pheno datasets

> See the [Jupyter Notebook version here](https://github.com/laderast/xvhelper/blob/master/vignettes/articles/downloading-pheno-data.ipynb)

If we are running JupyterLab or RStudio on the RAP/DNAnexus platform, we can use `xvhelper` to automate our downloading and decoding of the Pheno Data.

## Install `xvhelper` if necessary

```{r eval=FALSE}
install.packages("remotes")
remotes::install_github("laderast/xvhelper")
```

## UKB RAP RStudio only

The RStudio version on UKB RAP needs an updated dx-toolkit to use this functionality. You can run the code below to update it and install pandas.

```{bash eval=FALSE}
pip3 install dxpy==0.354.0
pip3 install pandas
```

```{r eval=FALSE}
reticulate::use_python("/usr/bin/python3")
```


## Starting Out

```{r}
library(xvhelper)
```

## Start with the Datasets

We can first use `find_all_datasets()` to return a `data.frame` of all datasets available in our project.

```{r}
datasets <- find_all_datasets()
datasets
```

On UKB RAP, you'll see that the dataset name follows the following convention:

`{application_id}_{date_dispensed}.dataset`

We will use the latest dataset, which is the top row (well, there is only one dataset in our project, but if you do multiple dispensals in your project you will have multiple datasets). We can also find this by using `find_dataset_id`, which will give us the last dataset dispensed:

```{r}
ds_id <- find_dataset_id()
ds_id
```

Now we have our project/dataset id, we can use it to grab metadata. We'l first fetch the dictionaries for our particular dataset.

```{r}
get_dictionaries(ds_id)
```

Now that we have the dictionary files into our JupyterLab/RStudio storage, we can extract the coding/data dictionary, which we'll use in our decoding.

```{r}
codings <- get_coding_table(ds_id)
head(codings)
```

In the next step, we'll need a list of fields. We can explore the list of fields as a searchable table using `explore_field_list()`:

```{r}
explore_field_list(ds_id)
```

## Extracting Data

Now that we have the dataset id, we can extract the data into our RStudio Project. By default, `extract_data()` will save the data as a file into our current working directory.

```{r}
fields <- c("participant.eid", "participant.p31", "participant.p41202")
extract_data(ds_id, fields)
```

Let's read in the data file in. 

```{r}
#| message: false
ds_name <- get_name_from_full_id(ds_id)

ds_file <- glue::glue("{ds_name}.data.csv")

data <- readr::read_csv(ds_file, show_col_types = FALSE)
head(data)
```

```{r}
decoded_data <- data[1:5000,] |>
  decode_single(codings) |>
  decode_multi_large_df(codings) |>
  decode_column_names(codings) 
```

```{r}
head(decoded_data)
```

## Reading in Cohort Information

Working with cohorts is very similar to working with the entire dataset. Let's list the cohorts in our project:

```{r}
cohorts <- find_all_cohorts()
cohorts
```

Once we have the cohort `record` IDs, we can use `extract_data()` to extract the cohorts to our project.

```{r}
fields <- c("participant.eid", "participant.p31", "participant.p41202")
cohort_id <- cohorts$project_record[1]
extract_data(cohort_id, fields)
```

```{r}
cohort1 <- readr::read_csv("female_control_3.0.data.csv")
knitr::kable(cohort1[1:10,])
```

We can decode our cohort in the same way:

```{r}
cohort1[1:10,] |>
  decode_single(codings) |>
  decode_multi_purrr(codings) |>
  decode_column_names(codings)
```

## Limitations

`extract_data()` is based on `dx extract_dataset` from the `dx-toolkit`. There are some limitations with queries, since it uses a shared resource called the Thrift server to extract data. You may get an error if you try to extract more than ~15 fields at once.

I am working on functions that run `Table Exporter` for much larger sets of fields.
